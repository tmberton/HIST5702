These are my notes for the Programming Historian tutorials for February 1.

#### Automated Downloading with Wget

So, this is meant to teach you how to use Wget, which is used to pull information from a website, either to take information from it or to copy the whole thing.

Recommends caution, since this is a powerful tool. Build-in limits and delays to not overload servers.

Download instructions. For Windows, recommends putting it in the Windows directory to be able to use it anywhere. Hmm, recommends the 32-bit system, even though I'm on a 64-bit OS. I guess I'll go with the recommendation though.

For some reason my computer isn't letting me save in C:Windows, even though I'm running as an Administrator. I'll try putting it in my Downloads folder and then moving it. There we go, that worked.

Alright, so now I have it downloaded in the right directory.

Made a wget directory in my Research Notebook and then ran the command by copying it from the tutorial. It worked!

Learning extra commands.

-r: "Recursive retrieval" has the program follow links and download them too - any links, so be careful!

--no-parent (-np): this will make it follow links, but not past the parent directory, essentially staying on the same website.

Then you can add numbers to be more specific about the number of links you want it to follow.

-w followed by a number adds a wait between requests to not overload the server.

--limit-rate= limits your bandwith, again to avoid ovverloading.

Now we're going to download all the ActiveHistory papers. I should actually type all this in to make sure I understand it. Good tip about trailing slash indicating something is a directory rather than a file. Order of options doesn't matter, also a good tip.

I typed this
wget -r --no-parent -w 2 --limit-rate=20k http://activehistory.ca/papers/

As promised, it's slower, but seems to be working. It worked! Oh wait, actually, looking through the log it seems like a bunch of it failed. Although, every spot where it failed said that the name was right, there just wasn't data of that type there so maybe it was just looking for some stuff that doesn't exist and that's fine.

-m lets you mirror a whole website. Don't think I'll actually do that right now. Although, actually, that's the end of the tutorial, so I may as well try it
